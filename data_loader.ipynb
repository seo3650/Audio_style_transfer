{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from torch.utils import data\n",
        "import torch\n",
        "import os\n",
        "import random\n",
        "import glob\n",
        "from os.path import join, basename, dirname, split\n",
        "import numpy as np\n",
        "\n",
        "# Below is the accent info for the used 10 speakers.\n",
        "spk2acc = {'262': 'Edinburgh', #F\n",
        "           '272': 'Edinburgh', #M\n",
        "           '229': 'SouthEngland', #F \n",
        "           '232': 'SouthEngland', #M\n",
        "           '292': 'NorthernIrishBelfast', #M \n",
        "           '293': 'NorthernIrishBelfast', #F \n",
        "           '360': 'AmericanNewJersey', #M\n",
        "           '361': 'AmericanNewJersey', #F\n",
        "           '248': 'India', #F\n",
        "           '251': 'India'} #M\n",
        "min_length = 256   # Since we slice 256 frames from each utterance when training.\n",
        "# Build a dict useful when we want to get one-hot representation of speakers.\n",
        "speakers = ['p262', 'p272', 'p229', 'p232', 'p292', 'p293', 'p360', 'p361', 'p248', 'p251']\n",
        "spk2idx = dict(zip(speakers, range(len(speakers))))\n",
        "\n",
        "def to_categorical(y, num_classes=None):\n",
        "    \"\"\"Converts a class vector (integers) to binary class matrix.\n",
        "    E.g. for use with categorical_crossentropy.\n",
        "    # Arguments\n",
        "        y: class vector to be converted into a matrix\n",
        "            (integers from 0 to num_classes).\n",
        "        num_classes: total number of classes.\n",
        "    # Returns\n",
        "        A binary matrix representation of the input. The classes axis\n",
        "        is placed last.\n",
        "    From Keras np_utils\n",
        "    \"\"\"\n",
        "    y = np.array(y, dtype='int')\n",
        "    input_shape = y.shape\n",
        "    if input_shape and input_shape[-1] == 1 and len(input_shape) > 1:\n",
        "        input_shape = tuple(input_shape[:-1])\n",
        "    y = y.ravel()\n",
        "    if not num_classes:\n",
        "        num_classes = np.max(y) + 1\n",
        "    n = y.shape[0]\n",
        "    categorical = np.zeros((n, num_classes), dtype=np.float32)\n",
        "    categorical[np.arange(n), y] = 1\n",
        "    output_shape = input_shape + (num_classes,)\n",
        "    categorical = np.reshape(categorical, output_shape)\n",
        "    return categorical\n",
        "\n",
        "class MyDataset(data.Dataset):\n",
        "    \"\"\"Dataset for MCEP features and speaker labels.\"\"\"\n",
        "    def __init__(self, data_dir):\n",
        "        mc_files = glob.glob(join(data_dir, '*.npy'))\n",
        "        mc_files = [i for i in mc_files if basename(i)[:4] in speakers] \n",
        "        self.mc_files = self.rm_too_short_utt(mc_files)\n",
        "        self.num_files = len(self.mc_files)\n",
        "        print(\"\\t Number of training samples: \", self.num_files)\n",
        "        for f in self.mc_files:\n",
        "            mc = np.load(f)\n",
        "            if mc.shape[0] <= min_length:\n",
        "                print(f)\n",
        "                raise RuntimeError(f\"The data may be corrupted! We need all MCEP features having more than {min_length} frames!\") \n",
        "\n",
        "    def rm_too_short_utt(self, mc_files, min_length=min_length):\n",
        "        new_mc_files = []\n",
        "        for mcfile in mc_files:\n",
        "            mc = np.load(mcfile)\n",
        "            if mc.shape[0] > min_length:\n",
        "                new_mc_files.append(mcfile)\n",
        "        return new_mc_files\n",
        "\n",
        "    def sample_seg(self, feat, sample_len=min_length):\n",
        "        assert feat.shape[0] - sample_len >= 0\n",
        "        s = np.random.randint(0, feat.shape[0] - sample_len + 1)\n",
        "        return feat[s:s+sample_len, :]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_files\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        filename = self.mc_files[index]\n",
        "        spk = basename(filename).split('_')[0]\n",
        "        spk_idx = spk2idx[spk]\n",
        "        mc = np.load(filename)\n",
        "        mc = self.sample_seg(mc)\n",
        "        mc = np.transpose(mc, (1, 0))  # (T, D) -> (D, T), since pytorch need feature having shape\n",
        "        # to one-hot\n",
        "        spk_cat = np.squeeze(to_categorical([spk_idx], num_classes=len(speakers)))\n",
        "\n",
        "        return torch.FloatTensor(mc), torch.LongTensor([spk_idx]).squeeze_(), torch.FloatTensor(spk_cat)\n",
        "        \n",
        "\n",
        "class TestDataset(object):\n",
        "    \"\"\"Dataset for testing.\"\"\"\n",
        "    def __init__(self, data_dir, wav_dir, src_spk='p262', trg_spk='p272'):\n",
        "        self.src_spk = src_spk\n",
        "        self.trg_spk = trg_spk\n",
        "        self.mc_files = sorted(glob.glob(join(data_dir, '{}*.npy'.format(self.src_spk))))\n",
        "\n",
        "        self.src_spk_stats = np.load(join(data_dir.replace('test', 'train'), '{}_stats.npz'.format(src_spk)))\n",
        "        self.trg_spk_stats = np.load(join(data_dir.replace('test', 'train'), '{}_stats.npz'.format(trg_spk)))\n",
        "        \n",
        "        self.logf0s_mean_src = self.src_spk_stats['log_f0s_mean']\n",
        "        self.logf0s_std_src = self.src_spk_stats['log_f0s_std']\n",
        "        self.logf0s_mean_trg = self.trg_spk_stats['log_f0s_mean']\n",
        "        self.logf0s_std_trg = self.trg_spk_stats['log_f0s_std']\n",
        "        self.mcep_mean_src = self.src_spk_stats['coded_sps_mean']\n",
        "        self.mcep_std_src = self.src_spk_stats['coded_sps_std']\n",
        "        self.mcep_mean_trg = self.trg_spk_stats['coded_sps_mean']\n",
        "        self.mcep_std_trg = self.trg_spk_stats['coded_sps_std']\n",
        "        self.src_wav_dir = f'{wav_dir}/{src_spk}'\n",
        "        self.spk_idx = spk2idx[trg_spk]\n",
        "        spk_cat = to_categorical([self.spk_idx], num_classes=len(speakers))\n",
        "        self.spk_c_trg = spk_cat\n",
        "\n",
        "    def get_batch_test_data(self, batch_size=8):\n",
        "        batch_data = []\n",
        "        for i in range(batch_size):\n",
        "            mcfile = self.mc_files[i]\n",
        "            filename = basename(mcfile).split('-')[-1]\n",
        "            wavfile_path = join(self.src_wav_dir, filename.replace('npy', 'wav'))\n",
        "            batch_data.append(wavfile_path)\n",
        "        return batch_data       \n",
        "\n",
        "def get_loader(data_dir, batch_size=32, mode='train', num_workers=1):\n",
        "    dataset = MyDataset(data_dir)\n",
        "    data_loader = data.DataLoader(dataset=dataset,\n",
        "                                  batch_size=batch_size,\n",
        "                                  shuffle=(mode=='train'),\n",
        "                                  num_workers=num_workers,\n",
        "                                  drop_last=True)\n",
        "    return data_loader\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    loader = get_loader('./data/mc/train')\n",
        "    data_iter = iter(loader)\n",
        "    for i in range(10):\n",
        "        mc, spk_idx, acc_idx, spk_acc_cat = next(data_iter)\n",
        "        print('-'*50)\n",
        "        print(mc.size())\n",
        "        print(spk_idx.size())\n",
        "        print(acc_idx.size())\n",
        "        print(spk_acc_cat.size())\n",
        "        print(spk_idx.squeeze_())\n",
        "        print(spk_acc_cat)\n",
        "        print('-'*50)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}